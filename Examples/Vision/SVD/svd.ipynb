{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiming/anaconda3/envs/dmd2/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/yiming/anaconda3/envs/dmd2/lib/python3.8/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/yiming/anaconda3/envs/dmd2/lib/python3.8/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "/home/yiming/anaconda3/envs/dmd2/lib/python3.8/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:01<00:00,  3.41it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import StableVideoDiffusionPipeline\n",
    "from diffusers.utils import export_to_gif, export_to_video, load_image\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.video_logging import save_video_frames_as_frames_parallel, save_video_frames_as_frames, save_video_frames_as_mp4\n",
    "\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# device='cuda:2'\n",
    "device='cuda'\n",
    "weight_dtype = torch.float16\n",
    "\n",
    "pipeline = StableVideoDiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-video-diffusion-img2vid\", variant=\"fp16\"\n",
    ").to(device, weight_dtype)\n",
    "\n",
    "# # https://huggingface.co/docs/diffusers/v0.28.2/en/tutorials/fast_diffusion\n",
    "# if is_xformers_available():\n",
    "#     print(\"enable xformers memory efficient attention\")\n",
    "#     pipeline.unet.enable_xformers_memory_efficient_attention()\n",
    "# else:\n",
    "#     print(\"install xformers to enable memory efficient attention\")\n",
    "\n",
    "# pipeline.enable_model_cpu_offload()\n",
    "# 20-25% speedup\n",
    "# torch._inductor.config.conv_1x1_as_mm = True\n",
    "# torch._inductor.config.coordinate_descent_tuning = True\n",
    "# torch._inductor.config.epilogue_fusion = False\n",
    "# torch._inductor.config.coordinate_descent_check_all_directions = True\n",
    "# pipeline.unet.to(memory_format=torch.channels_last)\n",
    "# pipeline.vae.to(memory_format=torch.channels_last)\n",
    "# pipeline.unet = torch.compile(pipeline.unet, mode=\"max-autotune\", fullgraph=True)\n",
    "# pipeline.vae.decode = torch.compile(pipeline.vae.decode, mode=\"max-autotune\", fullgraph=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD unet params: 1524.623082 M\n",
      "SVD vae params: 34.163592 M\n",
      "SVD vae params: 63.579183 M\n",
      "SVD image encoder params: 632.0768 M\n"
     ]
    }
   ],
   "source": [
    "print(f'SVD unet params: {sum([p.numel() for p in pipeline.unet.parameters()]) / 1e6} M')\n",
    "print(f'SVD vae encoder params: {sum([p.numel() for p in pipeline.vae.encoder.parameters()]) / 1e6} M')\n",
    "print(f'SVD vae decoder params: {sum([p.numel() for p in pipeline.vae.decoder.parameters()]) / 1e6} M')\n",
    "print(f'SVD image encoder params: {sum([p.numel() for p in pipeline.image_encoder.parameters()]) / 1e6} M')\n",
    "# test 1-step inference speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 1-step inference speed\n",
    "img = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/svd/rocket.png\")\n",
    "generator = torch.manual_seed(42)\n",
    "for _ in range(10):\n",
    "    frames = pipeline(img, decode_chunk_size=7, generator=generator, motion_bucket_id=127, fps=7, num_inference_steps=1).frames[0]\n",
    "# export_to_gif(frames, \"generated_1step.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.24it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.24it/s]\n",
      "100%|██████████| 8/8 [00:06<00:00,  1.24it/s]\n",
      "100%|██████████| 16/16 [00:12<00:00,  1.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.24it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.24it/s]\n",
      "100%|██████████| 8/8 [00:06<00:00,  1.24it/s]\n",
      "100%|██████████| 16/16 [00:12<00:00,  1.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.24it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.24it/s]\n",
      "100%|██████████| 8/8 [00:06<00:00,  1.24it/s]\n",
      "100%|██████████| 16/16 [00:12<00:00,  1.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.24it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.24it/s]\n",
      "100%|██████████| 8/8 [00:06<00:00,  1.24it/s]\n",
      "100%|██████████| 16/16 [00:12<00:00,  1.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.24it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.24it/s]\n",
      "100%|██████████| 8/8 [00:06<00:00,  1.24it/s]\n",
      "100%|██████████| 16/16 [00:12<00:00,  1.24it/s]\n",
      "100%|██████████| 25/25 [00:20<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# img = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/svd/rocket.png\")\n",
    "# img_names = [\n",
    "#     \"bird-8014191_1280.jpg\",\n",
    "#     \"dog-7396912_1280.jpg\",\n",
    "#     \"girl-4898696_1280.jpg\",\n",
    "#     \"leaf-7260246_1280.jpg\",\n",
    "#     \"power-station-6579092_1280.jpg\",\n",
    "#     \"rocket.png\",\n",
    "#     \"training-8122941_1280.jpg\",\n",
    "#     \"woman-4549327_1280.jpg\",\n",
    "#     \"woman-5667299_1280.jpg\"\n",
    "# ]\n",
    "img_names = os.listdir('../assets/images')\n",
    "output_dir = 'svd'\n",
    "generator = torch.manual_seed(42)\n",
    "for name in img_names:\n",
    "    img = load_image(f\"../assets/images/{name}\")\n",
    "    img = img.resize(576, 1024)\n",
    "\n",
    "    # motion_bucket_id: The higher the number the more motion will be in the video.\n",
    "    # fps: The higher the fps the less choppy the video will be.\n",
    "    for i in [4, 8, 16, 25]:\n",
    "        with torch.no_grad():\n",
    "            for max_guidance_scale in [2.5]: #[1.1, 1.5, 2.0, 3.0]:\n",
    "                # default max_guidance_scale=3.0, do_cfg is True, therefore bs=2\n",
    "                frames = pipeline(img, decode_chunk_size=14, generator=generator, max_guidance_scale=max_guidance_scale, motion_bucket_id=127, fps=7, num_inference_steps=i).frames[0]\n",
    "                export_to_video(frames, f\"{output_dir}/generated_{name[:-4]}_step_{i}_max_{max_guidance_scale}.gif\")\n",
    "# frames = pipeline(img, decode_chunk_size=8, generator=generator).frames[0]\n",
    "# export_to_gif(frames, \"generated_1step.gif\")\n",
    "# export_to_video(frames, \"rocket.mp4\", fps=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"generated.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# Replace 'generated.gif' with the path to your GIF file\n",
    "gif_path = 'generated.gif'\n",
    "Image(url=gif_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:20<00:00,  1.24it/s]\n",
      " 60%|██████    | 15/25 [00:12<00:08,  1.24it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "ucf_frames = '/home/yiming/project/MyProjects/VisionGen/SVD/ucf-101_frames'\n",
    "\n",
    "def process_ucf_frames(ucf_frames, num_inference_steps=4, max_process=10):\n",
    "    ucf_generated_svd = f'./ucf_generated_svd_{num_inference_steps}steps'\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(ucf_generated_svd, exist_ok=True)\n",
    "\n",
    "    # Get a list of all image files in the ucf_frames directory\n",
    "    image_files = []\n",
    "    for root, dirs, files in os.walk(ucf_frames):\n",
    "        for file in files:\n",
    "            if file.endswith('.jpg'):  # Assuming the frames are saved as .jpg\n",
    "                image_files.append(os.path.join(root, file))\n",
    "\n",
    "    num_processed = 0\n",
    "    # Process each image file\n",
    "    for image_file in image_files:\n",
    "        img = Image.open(image_file)\n",
    "        img = img.resize((1024, 576))\n",
    "        generator = torch.manual_seed(42)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            frames = pipeline(img, decode_chunk_size=7, generator=generator, motion_bucket_id=127, fps=7, num_inference_steps=num_inference_steps).frames[0]\n",
    "            resized_frames = [frame.resize((320, 240)) for frame in frames]\n",
    "            output_gif_path = os.path.join(ucf_generated_svd, f\"{os.path.basename(image_file)[:-4]}.mp4\")\n",
    "            export_to_video(resized_frames, output_gif_path, fps=7)\n",
    "        num_processed += 1\n",
    "        if num_processed > max_process:\n",
    "            break\n",
    "\n",
    "# Example usage\n",
    "# process_ucf_frames(ucf_frames)\n",
    "# process_ucf_frames(ucf_frames, num_inference_steps=8)\n",
    "# process_ucf_frames(ucf_frames, num_inference_steps=16)\n",
    "# process_ucf_frames(ucf_frames, num_inference_steps=25, max_process=200000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
